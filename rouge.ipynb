{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "# from summarize import processBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
    "\n",
    "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4786324739396596\n",
      "0.20833333333333334\n",
      "0.5277777777777778\n"
     ]
    }
   ],
   "source": [
    "# To extract the F1-Score from the ROUGE-1 Unigram method\n",
    "print(scores[0]['rouge-1']['f'])\n",
    "\n",
    "# To extract the Recall from the ROUGE-2 Bigram method\n",
    "print(scores[0]['rouge-2']['r'])\n",
    "\n",
    "# To extract the Precision from the ROUGE-L Longest Common Sequence method\n",
    "print(scores[0]['rouge-l']['p'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def processBook(fname):\n",
    "    \"\"\" Function to process .txt books, esp. from Gutenberg.\n",
    "    PARAMS: fname (str) - filepath to book to be processed into a string\n",
    "    RETURNS: (str) - the string object containing the text\n",
    "    \"\"\"\n",
    "    book = open(fname)      # open book file\n",
    "    book_lines = []         # initialize storage\n",
    "    for line in book.readlines():       # iterate through each line\n",
    "        book_lines.append(line.strip())     # process each line\n",
    "\n",
    "    return \" \".join(book_lines)         # return processed lines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def evaluateSummary(controltext1, controltext2, summarytext):\n",
    "    \"\"\"Helper function to run summarization experiments\n",
    "    PARAMS: controltext1 (str) - path to Cliff's Notes summary\n",
    "            controltext2 (str) - path to Grade Saver summary\n",
    "            summarytext (str) - path to generated summary to evaluate\n",
    "    \"\"\"\n",
    "    control1 = processBook(controltext1)\n",
    "    control2 = processBook(controltext2)\n",
    "    summary = processBook(summarytext)\n",
    "\n",
    "    # Calculate the upper bound (ROUGE of 2 manually created summaries)\n",
    "    ub_scores = rouge.get_scores(control1, control2)\n",
    "    sum_scores_cn = rouge.get_scores(control1, summary)\n",
    "    sum_scores_gs = rouge.get_scores(control2, summary)\n",
    "    print(\"Scores for {}\".format(controltext1[18:]))\n",
    "    print(\"Upper Bound:\")\n",
    "    printROUGEscores(ub_scores)\n",
    "    print(\"\\nCliff's Notes:\")\n",
    "    printROUGEscores(sum_scores_cn)\n",
    "    print(\"GradeSaver:\")\n",
    "    printROUGEscores(sum_scores_gs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def printROUGEscores(scores):\n",
    "    counter = 1         # counter in case of multiple scores\n",
    "    for scoring in scores:\n",
    "        print(\"Scores for Phrase {}\".format(counter))\n",
    "        counter += 1\n",
    "        for method in scoring:\n",
    "            for score in scoring[method]:\n",
    "                # print(\"Method: {}\\t\\t{}: {:.3%}\".format(method, score, scoring[method][score]))\n",
    "                print(scoring[method][score])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell contains the ROUGE scoring for the comparison between the Cliff's Notes summary (`cn_alice.txt`)\n",
    "and the Grade Saver summary (`gs_alice.txt`). This should be considered an 'upper bounds' of sort,\n",
    "since it is unreasonable to expect our summarizer to create a better summary than\n",
    "a human.\n",
    "\n",
    "The ROUGE-1 uses unigram tokens, so it tests recall, precision and f-score for\n",
    "individual tokens.\n",
    "\n",
    "ROUGE-2 uses bigrams, so two-token sequences (e.g. \"my dog\", \"dog is\", \"is a\", \"a great\", etc.).\n",
    "\n",
    "ROUGE-L uses longest common sequences within the strings.\n",
    "\n",
    "More documentation at https://pypi.org/project/rouge/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ub_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-38-a3927fa578e8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprintROUGEscores\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mub_scores\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ub_scores' is not defined"
     ]
    }
   ],
   "source": [
    "printROUGEscores(ub_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluateSummary(\"texts/controls/cn_alice.txt\",\n",
    "                \"texts/controls/gs_alice.txt\",\n",
    "                \"texts/summaries/whole_summary_alice.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "evaluateSummary(\"texts/controls/cn_frank.txt\",\n",
    "                \"texts/controls/gs_frank.txt\",\n",
    "                \"texts/summaries/whole_summary_frank.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluateSummary(\"texts/controls/cn_pride.txt\",\n",
    "                \"texts/controls/gs_pride.txt\",\n",
    "                \"texts/summaries/whole_summary_pride.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for tale.txt\n",
      "Upper Bound:\n",
      "Scores for Phrase 1\n",
      "0.48510637817629926\n",
      "0.40466531440162273\n",
      "0.6054628224582701\n",
      "0.12172854054194042\n",
      "0.10152284263959391\n",
      "0.1519756838905775\n",
      "0.31794871308336625\n",
      "0.27312775330396477\n",
      "0.3803680981595092\n",
      "\n",
      "Cliff's Notes:\n",
      "Scores for Phrase 1\n",
      "0.20848938353138488\n",
      "0.16937119675456389\n",
      "0.2711038961038961\n",
      "0.02749999526738363\n",
      "0.02233502538071066\n",
      "0.03577235772357724\n",
      "0.13863927626676528\n",
      "0.11894273127753303\n",
      "0.16615384615384615\n",
      "GradeSaver:\n",
      "Scores for Phrase 1\n",
      "0.2509803871625498\n",
      "0.24279210925644917\n",
      "0.2597402597402597\n",
      "0.0392772927276223\n",
      "0.037993920972644375\n",
      "0.04065040650406504\n",
      "0.15360982602919784\n",
      "0.15337423312883436\n",
      "0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "evaluateSummary(\"texts/controls/cn_tale.txt\",\n",
    "                \"texts/controls/gs_tale.txt\",\n",
    "                \"texts/summaries/whole_summary_tale.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for treasure.txt\n",
      "Upper Bound:\n",
      "Scores for Phrase 1\n",
      "0.45317545249138225\n",
      "0.43357363542739447\n",
      "0.4746335963923337\n",
      "0.09913792604472473\n",
      "0.09484536082474226\n",
      "0.1038374717832957\n",
      "0.2709677369477166\n",
      "0.2850678733031674\n",
      "0.2581967213114754\n",
      "\n",
      "Cliff's Notes:\n",
      "Scores for Phrase 1\n",
      "0.2027832956104944\n",
      "0.15756951596292482\n",
      "0.2843866171003718\n",
      "0.015925675572038736\n",
      "0.012371134020618556\n",
      "0.0223463687150838\n",
      "0.15068492672936776\n",
      "0.1244343891402715\n",
      "0.1909722222222222\n",
      "GradeSaver:\n",
      "Scores for Phrase 1\n",
      "0.2091228023174541\n",
      "0.16798196166854565\n",
      "0.27695167286245354\n",
      "0.019676734583959375\n",
      "0.01580135440180587\n",
      "0.0260707635009311\n",
      "0.14948453141460316\n",
      "0.11885245901639344\n",
      "0.2013888888888889\n"
     ]
    }
   ],
   "source": [
    "evaluateSummary(\"texts/controls/cn_treasure.txt\",\n",
    "                \"texts/controls/gs_treasure.txt\",\n",
    "                \"texts/summaries/whole_summary_treasure.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for warpeace.txt\n",
      "Upper Bound:\n",
      "Scores for Phrase 1\n",
      "0.4155929916001533\n",
      "0.53035413153457\n",
      "0.3416621401412276\n",
      "0.08528925143277126\n",
      "0.10886075949367088\n",
      "0.07010869565217391\n",
      "0.24684014375076088\n",
      "0.27483443708609273\n",
      "0.22402159244264508\n",
      "\n",
      "Cliff's Notes:\n",
      "Scores for Phrase 1\n",
      "0.15067686453085838\n",
      "0.10792580101180438\n",
      "0.24951267056530213\n",
      "0.022392453063936626\n",
      "0.016033755274261603\n",
      "0.037109375\n",
      "0.10091742693502248\n",
      "0.0728476821192053\n",
      "0.16417910447761194\n",
      "GradeSaver:\n",
      "Scores for Phrase 1\n",
      "0.14783347152758305\n",
      "0.09451385116784357\n",
      "0.3391812865497076\n",
      "0.018707479587209655\n",
      "0.011956521739130435\n",
      "0.04296875\n",
      "0.12289395050908535\n",
      "0.08367071524966262\n",
      "0.23134328358208955\n"
     ]
    }
   ],
   "source": [
    "evaluateSummary(\"texts/controls/cn_warpeace.txt\",\n",
    "                \"texts/controls/gs_warpeace.txt\",\n",
    "                \"texts/summaries/whole_summary_warpeace.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for huckfin.txt\n",
      "Upper Bound:\n",
      "Scores for Phrase 1\n",
      "0.36989072845034787\n",
      "0.6566091954022989\n",
      "0.25746478873239437\n",
      "0.11340623329798222\n",
      "0.2014388489208633\n",
      "0.07891770011273957\n",
      "0.2686849531038829\n",
      "0.4251497005988024\n",
      "0.19640387275242047\n",
      "\n",
      "Cliff's Notes:\n",
      "Scores for Phrase 1\n",
      "0.18336162504616338\n",
      "0.15517241379310345\n",
      "0.22406639004149378\n",
      "0.023809518975095267\n",
      "0.02014388489208633\n",
      "0.029106029106029108\n",
      "0.12751677359645983\n",
      "0.11377245508982035\n",
      "0.1450381679389313\n",
      "GradeSaver:\n",
      "Scores for Phrase 1\n",
      "0.14798404626437642\n",
      "0.09408450704225352\n",
      "0.34647302904564314\n",
      "0.015077601965403082\n",
      "0.009582863585118376\n",
      "0.035343035343035345\n",
      "0.136040605232271\n",
      "0.09266943291839558\n",
      "0.25572519083969464\n"
     ]
    }
   ],
   "source": [
    "evaluateSummary(\"texts/controls/cn_huckfin.txt\",\n",
    "                \"texts/controls/gs_huckfin.txt\",\n",
    "                \"texts/summaries/whole_summary_huckfin.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for david.txt\n",
      "Upper Bound:\n",
      "Scores for Phrase 1\n",
      "0.4104116179848405\n",
      "0.658252427184466\n",
      "0.29815303430079154\n",
      "0.12121211692265213\n",
      "0.19455252918287938\n",
      "0.0880281690140845\n",
      "0.3342105216831372\n",
      "0.4584837545126354\n",
      "0.2629399585921325\n",
      "\n",
      "Cliff's Notes:\n",
      "Scores for Phrase 1\n",
      "0.2578796511617767\n",
      "0.2621359223300971\n",
      "0.25375939849624063\n",
      "0.015310999786013857\n",
      "0.01556420233463035\n",
      "0.015065913370998116\n",
      "0.17314487132733597\n",
      "0.17689530685920576\n",
      "0.1695501730103806\n",
      "GradeSaver:\n",
      "Scores for Phrase 1\n",
      "0.2192929854712642\n",
      "0.16094986807387862\n",
      "0.34398496240601506\n",
      "0.01679663633044829\n",
      "0.01232394366197183\n",
      "0.026365348399246705\n",
      "0.17098445127429612\n",
      "0.13664596273291926\n",
      "0.22837370242214533\n"
     ]
    }
   ],
   "source": [
    "evaluateSummary(\"texts/controls/cn_david.txt\",\n",
    "                \"texts/controls/gs_david.txt\",\n",
    "                \"texts/summaries/whole_summary_david.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for expectations.txt\n",
      "Upper Bound:\n",
      "Scores for Phrase 1\n",
      "0.5323116172327947\n",
      "0.6918990703851262\n",
      "0.43254462432544627\n",
      "0.1620240177554111\n",
      "0.2106312292358804\n",
      "0.13164451827242524\n",
      "0.34721241578710776\n",
      "0.42857142857142855\n",
      "0.2918149466192171\n",
      "\n",
      "Cliff's Notes:\n",
      "Scores for Phrase 1\n",
      "0.13669820911091438\n",
      "0.08632138114209828\n",
      "0.3282828282828283\n",
      "0.017894733548615562\n",
      "0.011295681063122924\n",
      "0.043037974683544304\n",
      "0.11913814559155447\n",
      "0.08188153310104529\n",
      "0.2186046511627907\n",
      "GradeSaver:\n",
      "Scores for Phrase 1\n",
      "0.10338680684424878\n",
      "0.06019095060190951\n",
      "0.3661616161616162\n",
      "0.01712450703291918\n",
      "0.009966777408637873\n",
      "0.060759493670886074\n",
      "0.10775046935143538\n",
      "0.06761565836298933\n",
      "0.2651162790697674\n"
     ]
    }
   ],
   "source": [
    "evaluateSummary(\"texts/controls/cn_expectations.txt\",\n",
    "                \"texts/controls/gs_expectations.txt\",\n",
    "                \"texts/summaries/whole_summary_expectations.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}