{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extractive_text_summarization.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8nw3lPeIZM9",
        "outputId": "08920feb-3afd-4e1d-9953-00cc2fff00c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.7)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcYULwyJIbVX"
      },
      "source": [
        "import stanza, nltk\n",
        "from collections import Counter\n",
        "from string import punctuation"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL2uFUcEIgFm",
        "outputId": "19fca6b8-19c4-4138-b516-3dc043550728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download Stanza LM\n",
        "stanza.download('en')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 10.3MB/s]                    \n",
            "2020-11-05 21:57:52 INFO: Downloading default packages for language: en (English)...\n",
            "2020-11-05 21:57:53 INFO: File exists: /root/stanza_resources/en/default.zip.\n",
            "2020-11-05 21:57:59 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYZZqCRGIj7S",
        "outputId": "0b6ebbb7-d6ef-4c43-98b5-f4041c30a891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Initialize Stanza Pipeline\n",
        "nlp = stanza.Pipeline('en', processors='tokenize,pos')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-05 22:25:37 INFO: Loading these models for language: en (English):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ewt     |\n",
            "| pos       | ewt     |\n",
            "=======================\n",
            "\n",
            "2020-11-05 22:25:37 INFO: Use device: cpu\n",
            "2020-11-05 22:25:37 INFO: Loading: tokenize\n",
            "2020-11-05 22:25:37 INFO: Loading: pos\n",
            "2020-11-05 22:25:38 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CohfFEwAbJNS"
      },
      "source": [
        "# Get a sample book from Gutenberg\n",
        "# !wget \"http://www.gutenberg.org/files/11/11-0.txt\"\n",
        "alice_str = open(\"/content/11-0.txt\").read()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvBVI1qekBA"
      },
      "source": [
        "# Initialize Stanza object from book\n",
        "doc = nlp(alice_str)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6SbfF9-J3Ef"
      },
      "source": [
        "#### From Tutorial\n",
        "\n",
        "https://medium.com/better-programming/extractive-text-summarization-using-spacy-in-python-88ab96d1fd97\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMLi43ieJGH_"
      },
      "source": [
        "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "\n",
        "TF: Term Frequency — Measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear many more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length, such as the total number of terms in the document, as a way of normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLyATkIWJOpZ"
      },
      "source": [
        "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "\n",
        "IDF: Inverse Document Frequency — Measures how important a term is. While computing the term frequency, all terms are considered equally important. However, it is known that certain terms may appear a lot of times but have little importance in the document. We usually term these words stopwords. For example: is, are, they, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SqoYEkuJGyW"
      },
      "source": [
        "def top_sentence(text, limit):\n",
        "    keyword = []\n",
        "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
        "    doc = nlp(text.lower())     # convert to stanza\n",
        "    for token in doc:\n",
        "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation): # convert to stanza\n",
        "            continue\n",
        "        if(token.pos_ in pos_tag): # convert to stanza\n",
        "            keyword.append(token.text)\n",
        "    \n",
        "    freq_word = Counter(keyword)\n",
        "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
        "    for w in freq_word:\n",
        "        freq_word[w] = (freq_word[w]/max_freq)\n",
        "        \n",
        "    sent_strength={}\n",
        "    for sent in doc.sents:\n",
        "        for word in sent:\n",
        "            if word.text in freq_word.keys():\n",
        "                if sent in sent_strength.keys():\n",
        "                    sent_strength[sent]+=freq_word[word.text]\n",
        "                else:\n",
        "                    sent_strength[sent]=freq_word[word.text]\n",
        "    \n",
        "    summary = []\n",
        "    \n",
        "    sorted_x = sorted(sent_strength.items(), key=lambda kv: kv[1], reverse=True)\n",
        "    \n",
        "    counter = 0\n",
        "    for i in range(len(sorted_x)):\n",
        "        summary.append(str(sorted_x[i][0]).capitalize())\n",
        "\n",
        "        counter += 1\n",
        "        if(counter >= limit):\n",
        "            break\n",
        "            \n",
        "    return ' '.join(summary)\n",
        "\n",
        "def main():\n",
        "  # TO DO: Need to add step to check for EN lang model\n",
        "  # Download Stanza LM\n",
        "  stanza.download('en')\n",
        "  # Initialize Stanza Pipeline\n",
        "  nlp = stanza.Pipeline('en', processors='tokenize,pos')\n",
        "  alice_str = open(\"/content/11-0.txt\").read()\n",
        "  top_sentence(alice_str, 30)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEmqE_iihWNQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}