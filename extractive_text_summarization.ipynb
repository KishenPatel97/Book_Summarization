{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "extractive_text_summarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "pycharm-4110bfb8",
   "language": "python",
   "display_name": "PyCharm (tml)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8nw3lPeIZM9",
    "outputId": "03c6016a-3514-4687-fcdf-c46dc4d6f857",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is just some messing around that I am doing with the stanza\n",
    "and the TF-IDF summarization tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bcYULwyJIbVX"
   },
   "source": [
    "import stanza\n",
    "from collections import Counter\n",
    "from string import punctuation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iL2uFUcEIgFm",
    "outputId": "c177b007-eb36-46a4-b336-6e59f465a3dc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Download English Language Model\n",
    "stanza.download('en')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 13.0MB/s]                    \n",
      "2020-11-05 15:59:20 INFO: Downloading default packages for language: en (English)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/en/default.zip: 100%|██████████| 428M/428M [00:13<00:00, 31.0MB/s]\n",
      "2020-11-05 15:59:41 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fYZZqCRGIj7S",
    "outputId": "cf030362-1320-488a-c347-dd470ce58019",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Initiate an English LM stanza pipeline\n",
    "nlp = stanza.Pipeline('en')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2020-11-05 15:59:47 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-11-05 15:59:47 INFO: Use device: cpu\n",
      "2020-11-05 15:59:47 INFO: Loading: tokenize\n",
      "2020-11-05 15:59:47 INFO: Loading: pos\n",
      "2020-11-05 15:59:48 INFO: Loading: lemma\n",
      "2020-11-05 15:59:48 INFO: Loading: depparse\n",
      "2020-11-05 15:59:49 INFO: Loading: sentiment\n",
      "2020-11-05 15:59:51 INFO: Loading: ner\n",
      "2020-11-05 15:59:51 INFO: Done loading processors!\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "InWHJI-vIqde"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6SbfF9-J3Ef"
   },
   "source": [
    "#### From Tutorial\n",
    "\n",
    "https://medium.com/better-programming/extractive-text-summarization-using-spacy-in-python-88ab96d1fd97\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMLi43ieJGH_"
   },
   "source": [
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "TF: Term Frequency — Measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear many more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length, such as the total number of terms in the document, as a way of normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLyATkIWJOpZ"
   },
   "source": [
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "IDF: Inverse Document Frequency — Measures how important a term is. While computing the term frequency, all terms are considered equally important. However, it is known that certain terms may appear a lot of times but have little importance in the document. We usually term these words stopwords. For example: is, are, they, and so on."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2SqoYEkuJGyW"
   },
   "source": [
    "def top_sentence(text, limit):\n",
    "    \"\"\"Function to read in a text and return a summary made from\n",
    "    the top (n=limit) sentence constituents\n",
    "    PARAMS: text (str) - book to be summarized\n",
    "            limit (int) - the number of the top sentences to use for the summary\n",
    "    RETURNS: (str) - the summary of the document, represented as the most\n",
    "                     important sentences by TF-IDF\n",
    "    \"\"\"\n",
    "    # Capture keywords from text\n",
    "    keyword = []        # initiate keyword list\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']   # Are these the only POS we want?\n",
    "    doc = nlp(text.lower())     # TO-DO: convert to stanza # normalize text and build stanza object\n",
    "    for token in doc:   # iterate through all document tokens\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation): # TO-DO: convert to stanza # ignore punctuation and stop_words\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag): # TO-DO: convert to stanza   # check against allowable tagset\n",
    "            keyword.append(token.text)          # append keyword to storage\n",
    "\n",
    "    # Calculate a normalixed value for each keyword\n",
    "    freq_word = Counter(keyword)            # count keywords\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]   # Capture frequency of most common word\n",
    "    for w in freq_word:\n",
    "        freq_word[w] = (freq_word[w]/max_freq)  # use most common freq to normalize all\n",
    "\n",
    "    # Calculate a cumulative normalized value for each sentence\n",
    "    sent_strength={}\n",
    "    for sent in doc.sents:  # TO-DO: convert to stanza # iterate through sentences\n",
    "        for word in sent:   # iterate through words in a sentence\n",
    "            if word.text in freq_word.keys():   # TO-DO: convert to stanza # check word against keywords\n",
    "                if sent in sent_strength.keys():  # Check if we have this key\n",
    "                    sent_strength[sent]+=freq_word[word.text]   # increment if we do\n",
    "                else:\n",
    "                    sent_strength[sent]=freq_word[word.text]    # create if we don't\n",
    "\n",
    "    # Aggregate Summary\n",
    "    summary = []        # initiate empty list for sentences\n",
    "    \n",
    "    sorted_x = sorted(sent_strength.items(), key=lambda kv: kv[1], reverse=True) # sort sentences by weight\n",
    "\n",
    "    counter = 0     # initialize counter variable\n",
    "    for i in range(len(sorted_x)):      # iterate through list of sorted sentences\n",
    "        summary.append(str(sorted_x[i][0]).capitalize())        # append the capitalized sentence to the list\n",
    "        counter += 1        # increment counter var\n",
    "        if(counter >= limit):       # early stop at the stated limit\n",
    "            break\n",
    "            \n",
    "    return ' '.join(summary)        # return string of top sentences"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}