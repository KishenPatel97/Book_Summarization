{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bleu.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTXoGo9gTK2kEtvakdL4A6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KishenPatel97/Book_Summarization/blob/main/bleu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjYxuFUbdGtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb020db-2108-4a91-8b27-52a555b09ab0"
      },
      "source": [
        "import nltk\n",
        "from nltk.translate import bleu\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "# from summarize import processBook"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aHedAtrvqqP",
        "outputId": "fc030c01-7417-489b-92cc-9ea9bfdce97a"
      },
      "source": [
        "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to help students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you saw on cnn student news\"\n",
        "\n",
        "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and vocabulary at the bottom of the page, comment for a chance to be mentioned on cnn student news. you must be a teacher or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests students' knowledge of even ts in the news\"\n",
        "\n",
        "hy1 = \"This text is just for testing purposes\"\n",
        "\n",
        "ref1 = \"The article is just meant for evaluation\"\n",
        "# Requires text in a tokenized format\n",
        "score1 = bleu([word_tokenize(reference)], word_tokenize(hypothesis))\n",
        "score1_2 = corpus_bleu([[word_tokenize(reference)]], [word_tokenize(hypothesis)])\n",
        "print(score1)\n",
        "print(score1_2)\n",
        "\n",
        "chencherry = SmoothingFunction()\n",
        "score2 = bleu([word_tokenize(ref1)], word_tokenize(hyp1), smoothing_function = chencherry.method1)\n",
        "print(score2)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.17607804141687589\n",
            "0.17607804141687589\n",
            "0.07730551756939454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL5NmEvdykVg"
      },
      "source": [
        "def processBook(fname):\n",
        "    \"\"\" Function to process .txt books, esp. from Gutenberg.\n",
        "    PARAMS: fname (str) - filepath to book to be processed into a string\n",
        "    RETURNS: (str) - the string object containing the text\n",
        "    \"\"\"\n",
        "    book = open(fname)      # open book file\n",
        "    book_lines = []         # initialize storage\n",
        "    for line in book.readlines():       # iterate through each line\n",
        "        book_lines.append(line.strip())     # process each line\n",
        "\n",
        "    return \" \".join(book_lines)         # return processed lines"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SZ_PpNzUe7W"
      },
      "source": [
        "def evaluateSummary(controltext1, controltext2, summarytext):\n",
        "    \"\"\"Helper function to run summarization experiments\n",
        "    PARAMS: controltext1 (str) - path to Cliff's Notes summary\n",
        "            controltext2 (str) - path to Grade Saver summary\n",
        "            summarytext (str) - path to generated summary to evaluate\n",
        "    \"\"\"\n",
        "    control1 = processBook(controltext1)\n",
        "    control2 = processBook(controltext2)\n",
        "    summary = processBook(summarytext)\n",
        "\n",
        "    chencherry = SmoothingFunction()\n",
        "\n",
        "    # Calculate the upper bound (Bleu of 2 manually created summaries)\n",
        "    ub_scores = bleu([word_tokenize(control1)], word_tokenize(control2), weights = (1, 0, 0, 0))\n",
        "    ub_scores2 = bleu([word_tokenize(control2)], word_tokenize(control1), weights = (1, 0, 0, 0))\n",
        "    \n",
        "    sum_scores_cn = bleu([word_tokenize(summary)], word_tokenize(control1), weights = (1, 0, 0, 0))\n",
        "    sum_scores_cn_bi = bleu([word_tokenize(summary)], word_tokenize(control1), weights = (0, 1, 0, 0))\n",
        "    sum_scores_gs = bleu([word_tokenize(summary)], word_tokenize(control2), weights = (1, 0, 0, 0))\n",
        "    sum_scores_gs_bi = bleu([word_tokenize(summary)], word_tokenize(control2), weights = (0, 1, 0, 0))\n",
        "\n",
        "    #print(\"Scores for {}\".format(controltext1[18:]))\n",
        "    print(f\"Scores for {controltext1}\")\n",
        "    print(\"Upper Bound:\")\n",
        "    print(ub_scores)\n",
        "    print(ub_scores2)\n",
        "    print(\"\\nCliff's Notes:\")\n",
        "    print(f'unigram: {sum_scores_cn}')\n",
        "    print(f'bigram:  {sum_scores_cn_bi}')\n",
        "    print(\"\\nGradeSaver:\")\n",
        "    print(f'unigram: {sum_scores_gs}')\n",
        "    print(f'bigram:  {sum_scores_gs_bi}')"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggObRxn3NPIf",
        "outputId": "af2913ae-0436-462e-ca6c-0aa908fb8349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluateSummary(\"cn_alice.txt\",\n",
        "                \"gs_alice.txt\",\n",
        "                \"whole_summary_alice.txt\")"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores for cn_alice.txt\n",
            "Upper Bound:\n",
            "0.08303134720918029\n",
            "0.2351590685470646\n",
            "\n",
            "Cliff's Notes:\n",
            "unigram: 0.1807149885208265\n",
            "bigram:  0.03608923884514437\n",
            "\n",
            "GradeSaver:\n",
            "unigram: 0.3405887852289164\n",
            "bigram:  0.06650345681735587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49IBx-RyXtvs",
        "outputId": "120050ac-7220-46ce-ac5f-6a52c0ab7470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluateSummary(\"cn_frank.txt\",\n",
        "                \"gs_frank.txt\",\n",
        "                \"whole_summary_frank.txt\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores for cn_frank.txt\n",
            "Upper Bound:\n",
            "0.2972972972972973\n",
            "0.17754126774891524\n",
            "\n",
            "Cliff's Notes:\n",
            "unigram: 0.28802154657946133\n",
            "bigram:  0.038299639348021566\n",
            "\n",
            "GradeSaver:\n",
            "unigram: 0.3\n",
            "bigram:  0.060387561964849026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTc4Lgrhz6lD",
        "outputId": "58ad7140-82da-40dc-ceeb-30393866b321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluateSummary(\"cn_pride.txt\",\n",
        "                \"gs_pride.txt\",\n",
        "                \"whole_summary_pride.txt\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores for cn_pride.txt\n",
            "Upper Bound:\n",
            "0.3231707317073171\n",
            "0.18140364266272244\n",
            "\n",
            "Cliff's Notes:\n",
            "unigram: 0.3282732447817837\n",
            "bigram:  0.03513770180436847\n",
            "\n",
            "GradeSaver:\n",
            "unigram: 0.1676829268292683\n",
            "bigram:  0.025162028211971027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r-f2VCW1jU4",
        "outputId": "39ab740c-babb-4d57-ae66-26b9fd5d05d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluateSummary(\"cn_tale.txt\",\n",
        "                \"gs_tale.txt\",\n",
        "                \"whole_summary_tale.txt\")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores for cn_tale.txt\n",
            "Upper Bound:\n",
            "0.39036978012722634\n",
            "0.43161634103019536\n",
            "\n",
            "Cliff's Notes:\n",
            "unigram: 0.25754884547069273\n",
            "bigram:  0.04266666666666667\n",
            "\n",
            "GradeSaver:\n",
            "unigram: 0.30169268682148603\n",
            "bigram:  0.05481650131126973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sexJMz89Vuz",
        "outputId": "474d4489-c51d-4ca2-d4ff-01a7462947e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluateSummary(\"cn_treasure.txt\",\n",
        "                \"gs_treasure.txt\",\n",
        "                \"whole_summary_treasure.txt\")"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores for cn_treasure.txt\n",
            "Upper Bound:\n",
            "0.49544922544410486\n",
            "0.4977738201246661\n",
            "\n",
            "Cliff's Notes:\n",
            "unigram: 0.2653606411398041\n",
            "bigram:  0.031194295900178255\n",
            "\n",
            "GradeSaver:\n",
            "unigram: 0.2595494613124388\n",
            "bigram:  0.03823529411764706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki-QWgrA9hjW",
        "outputId": "e194d60f-3e24-4fb5-a906-bd9a8f8f6761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "evaluateSummary(\"cn_huckfin.txt\",\n",
        "                \"gs_huckfin.txt\",\n",
        "                \"whole_summary_huckfin.txt\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-c94224c24cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m evaluateSummary(\"cn_huckfin.txt\",\n\u001b[1;32m      2\u001b[0m                 \u001b[0;34m\"gs_huckfin.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                 \"whole_summary_huckfin.txt\")\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-af5010296077>\u001b[0m in \u001b[0;36mevaluateSummary\u001b[0;34m(controltext1, controltext2, summarytext)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0msummarytext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0msummary\u001b[0m \u001b[0mto\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcontrol1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontroltext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcontrol2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontroltext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarytext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-2415e4480471>\u001b[0m in \u001b[0;36mprocessBook\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mRETURNS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# open book file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mbook_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m         \u001b[0;31m# initialize storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0;31m# iterate through each line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cn_huckfin.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13_X5ru_SoN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629784b0-312b-46f2-f07c-9adb3beeca64"
      },
      "source": [
        "# Requires text in a tokenized format\n",
        "score3 = bleu(word_tokenize(control1), word_tokenize(control2))\n",
        "print(score3)\n",
        "\n",
        "score3_rev = bleu(word_tokenize(control2), word_tokenize(control1))\n",
        "print(score3_rev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.311900088600225\n",
            "0.23930988821651522\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}